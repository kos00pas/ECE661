# 5. CNN

### Q1. What is a Convolutional Neural Network (CNN)?
- use convolution instead of general matrix multiplication in at least one layer


### Q2. What are the key advantages of using CNNs?
- sparse connectivity 
  - connect neurons to a local region of input rather than all neurons.
  - reduce parameters 
- parameter sharing : 
  - reuse filters across all spatial locations
  - decrease unique parameters 
  - feature detection regardless location

### Q3. Explain the convolution operation in CNNs.
-   dot product between the kernel (filter) and the input.

#### q what is feature map
-  contains the detected features at each spatial location in the input.

### Q4. What are the main types of layers in a CNN?
- COnv-> Actv. -> Pooling 
- COnv-> BN-> Actv. -> Pooling 

### Q5. How does pooling prevent overfitting in CNNs?
-  reduce the spatial dimensions of the feature map
- noise reduction
- focus on essential features

- => prevent overfitting and improve generalization


### Q6. What is the role of hyper-parameters in CNNs?
- **Padding:** Maintains spatial dimensions of the input.
- **Stride:** Controls the step size of the kernel during convolution.
- **Number of Kernels:** Defines the number of feature maps.
- **Kernel Size:** Determines the receptive field.


### Q13. Why are fully connected layers used after convolutional layers?
- combine all the features extracted


### Q14. Describe the difference between feature maps and activation maps.
- feature map is before activation map 

### Q15. How do deeper CNN layers learn hierarchical features?
- Early layers capture basic features like edges and corners.
- Middle layers capture patterns and textures.
- Final layers capture object-level information.

### Q18. What is the effect of stacking multiple convolutional and pooling layers?
-  Learning more complex patterns progressively.
- COnv-> BN-> Actv. -> Pooling


### Q19. How do CNNs leverage parameter efficiency compared to fully connected networks?
- **Parameter Sharing:** Same kernel is reused across the entire input.
- **Sparse Connectivity:** Each neuron connects to only a local input region.











