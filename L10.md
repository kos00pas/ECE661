# Generative Adversarial Networks (GANs): Theoretical Questions and Answers

---

### Questions Based on `.md` Notes

---

### Q1. What distinguishes generative models from discriminative models?

**A1.**
| **Aspect**               | **Discriminative Models**                    | **Generative Models**                         |
|--------------------------|---------------------------------------------|---------------------------------------------|
| **Focus**                | Estimates \( P(y|x) \): Conditional probability. | Estimates \( P(x) \): Joint or marginal probability. |
| **Goal**                 | Classify inputs into predefined categories.  | Generate new data samples.                   |
| **Examples**             | SVM, Logistic Regression.                   | GANs, Variational Autoencoders.              |

---

### Q2. What are the main components of a GAN?

**A2.**
1. **Generator (G):**
    - Creates synthetic data (e.g., images) from random noise.
    - Trains to fool the discriminator.
2. **Discriminator (D):**
    - Classifies inputs as real or fake.
    - Trains to distinguish between real and generated data.

---

### Q3. What is the role of the latent space in GANs?

**A3.**
- **Definition:** A low-dimensional representation used by the generator to produce high-dimensional outputs.
- **Purpose:** Encodes compressed information to generate diverse outputs.

---

### Q4. What are the main challenges in training GANs?

**A4.**
| **Issue**                | **Cause**                                  | **Solution**                           |
|--------------------------|-------------------------------------------|----------------------------------------|
| Training Instability     | Imbalance between G and D.                | Use WGAN or gradient clipping.         |
| Mode Collapse            | G focuses on limited outputs.             | Mini-batch discrimination or feature matching. |
| Vanishing Gradients      | D becomes too strong or weak.             | Use normalized architectures or balanced training. |

---

### Expanded Questions Based on `.pdf` Content

---

### Q5. How does the GAN training procedure work?

**A5.**
- **Objective:**
    - G maximizes D’s error by generating realistic samples.
    - D minimizes classification error by distinguishing real and fake data.
- **Training Steps:**
    1. Train D to classify real vs. fake samples.
    2. Train G to fool D by generating realistic samples.
    3. Alternate training until equilibrium.

---

### Q6. What is the minimax loss function in GANs?

**A6.**
- **Definition:**
    - \( \min_G \max_D V(G, D) = \mathbb{E}_{x \sim p_{\text{data}}} [\log D(x)] + \mathbb{E}_{z \sim p_z} [\log(1 - D(G(z)))] \)
- **Explanation:**
    - D tries to maximize its ability to classify real and fake samples.
    - G tries to minimize D’s ability to distinguish fake samples from real ones.

---

### Q7. What are common GAN variants and their use cases?

**A7.**
| **Variant**              | **Description**                            | **Use Case**                             |
|--------------------------|-------------------------------------------|----------------------------------------|
| **DCGAN**               | Uses CNNs for G and D.                    | Image generation and super-resolution. |
| **CycleGAN**            | Translates images between two domains.    | Style transfer, photo enhancement.     |
| **WGAN**                | Improves stability by using Wasserstein distance. | Reduces mode collapse and instability. |

---

### Q8. What are the primary applications of GANs?

**A8.**
- **Image Generation:** Generate realistic faces or textures.
- **Data Augmentation:** Create synthetic training data.
- **Anomaly Detection:** Detect outliers by comparing real and generated data.
- **Creative Applications:** Generate art, music, or design ideas.

---

### Q9. What are the major difficulties in GAN training?

**A9.**
1. **Training Instability:** Difficulty achieving convergence.
2. **Mode Collapse:** G produces limited diversity in outputs.
3. **Evaluation Metrics:** Lack of universal metrics like FID or IS.

---

### Q10. How do GANs differ from Variational Autoencoders (VAEs)?

**A10.**
| **Aspect**        | **GANs**                               | **VAEs**                                |
|--------------------|---------------------------------------|----------------------------------------|
| **Training**      | Adversarial training of G and D.      | Probabilistic framework with KL divergence. |
| **Output Quality**| High-quality outputs but unstable.    | Blurry outputs but stable training.    |
| **Latent Space**  | Implicitly learned via adversarial loss. | Explicitly defined and regularized.     |

---

Let me know if you'd like further expansions or refinements!
