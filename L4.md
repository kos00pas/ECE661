# Lecture 4  - Principles 


### Q1. What is the Universal Approximation Theorem in deep learning?

- a fw N can approximate any continues function  using one hidden layer

### Q2. Why are multiple hidden layers used in deep neural networks?

- tend to reduce the  generalization error 
- Deeper networks often require fewer parameters overall approximation 
- can represent more complex function 

### q - different from shallow(MLP) - to deep neural networks
- Shallow: few hidden layers  
- Deep : multiple hidden layers 

- Deep advantages 
  - learning hierarchical features
  - parameters efficiency 
  - generalization 
  - more practical and effective for complex representations 

### Q3. List and describe challenges faced in training deep networks.
- optimization complexity : training req solve non-convex
- vanishing/exploding gradients 
- increased computation
- weights& biases initialization 
- hyper-parameter tuning 
- architectural decision 
- choosing activation functions 



### Q4. Compare activation functions and their use cases.
- ReLu : mitigate vanishing 
- Leaky : prevent dying neurons 
- sigmoid : cause vanishing  : binary classification [0,1] - boundary
- tanh :improve converge - zero center - boundary 
- softmax : for output multi class , probabilities 



### q which are not good for address van/exp gradient 
- sigmoid -> vanishing
- tanh -> vanishing 
- linear ->  exploding
- exponential 


### Q6. Explain the vanishing/Exploding  gradient problem.
- During backpropagation, the gradient can become extremely small (vanishing) or excessively large (exploding).
- This can slow down or completely halt weight updates, preventing the model from learning effectively.



### q Techniques to Address Vanishing Gradients:
- use ReLu instead sigmoid/tanh 
- weight initialization methods e.g. xavier
- batch normalization 
- ResNets
- use shallower architectures 

### q Techniques to Address Exploding Gradients:
- gradient clipping 
- weight initialization methods e.g. xavier
- normalise  inputs 
- smaller learning rates 
- activation functions  with limited output range e.g sigmoid/tanh/softmax
- weight penalties like L2  

### Q5. What is the "rule of thumb" for the size of training data?
-  training  = (5~10) x weights


### Q10. How does weight initialization affect training?
- poor initialization 
  - slow convergence 
- proper 
  - help maintain signal  strength across layers  


### Q12. How does the depth of a network affect its performance?
 - hierarchical feature learning 
 - fewer neurons 
 - BUT:
   - need more data 
   - careful regularization  
   - more prune to vanishing/exploding gradients.



### Q15. How do dropout and regularization help prevent overfitting?
- dropout
  - prevent reliance on specific neurons 
  - help generalization by distributed learning - reduce redundancy 
- regularization
  -  penalizes large & unnecessary weights in the model,
  - encourages sparsity , effectively select important features -> some weights become zero


### Q16. What is the impact of the dataset size on model performance?
- Larger datasets improve generalization and reduce overfitting risks.


### Q17. How does the choice of optimizer affect training?
- Affects convergence speed and final accuracy.