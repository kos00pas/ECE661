### Q1. What is the role of the encoder and decoder in a Transformer architecture?
**Encoder:**
    - Encodes the input sequence into a fixed-length contextual representation.
    - Captures relationships and dependencies between elements in the input.
- **Decoder:**
    - Generates the output sequence step-by-step.
    - Leverages encoder's contextual representation and its own previous outputs.


### Q2. What is attention, and why is it important in Transformers?
- **Definition:**
    - Attention is a mechanism that dynamically focuses on the most relevant parts of the input sequence during decoding.
- **Importance:**
    - Handles long-range dependencies.
    - Avoids bottlenecks caused by fixed-length representations.
    - Enables efficient parallel computation.
  
### q How Attention Enables Efficient Parallel Computation
- processing the entire input sequence simultaneously, rather than sequentially as in RNNs
-  represent sequences as matrices
- captures global dependencies between all tokens simultaneously, eliminating the sequential dependency of RNNs.

### q RNNs and LSTMs VS attention 
Processing Style	Sequential: Processes input step-by-step.	Parallel: Processes all inputs simultaneously.
Contextual Representation	Fixed-length hidden state summarizing past inputs.	Dynamic: Each token gets a context-aware representation.


### Q3. What is the difference between attention and a Transformer?
- Attention : A mechanism that helps models focus on specific parts of input when making predictions.
- Transformer: A complete architecture that replaces RNNs and CNNs with attention mechanisms.



### Q4. What are the main types of attention in Transformers?
1. **Self-Attention:** - The input attends to itself, capturing relationships within the sequence.
2. **Encoder-Decoder Attention:** - The decoder attends to the encoder's output to align input and output sequences.
3. **Multi-Head Attention:** - Combines multiple attention mechanisms to capture diverse relationships.


### Q5. How does self-attention work in Transformers?
1. Create **Query (Q)**, **Key (K)**, and **Value (V)** matrices from input embeddings.
   - Values (V): The actual content (information) of each region.
   - Keys (K): What information does each region contain?
   - Queries (Q): What information are we looking for?
2. Compute similarity scores: \( Q \cdot K^T \).
3. Normalize scores using softmax.
4. Weight the **Value (V)** vectors by normalized scores.

### Q7. What is multi-head attention, and why is it used?
-  Runs multiple attention mechanisms in parallel.
- Combines their outputs for richer representations.

### Q6. Why is positional encoding needed in Transformers?
: Adds positional information to the embeddings to capture sequence order






### Q8. How does the Transformer architecture address vanishing gradients and long-range dependencies?
1. **Vanishing Gradients:** - Uses residual connections and layer normalization to stabilize training.
2. **Long-Range Dependencies:** - Self-attention allows direct connections between all tokens in the sequence.



### Q10. What are the key innovations introduced in the "Attention is All You Need" paper?

1. **Self-Attention Mechanism:**
    - Replaces recurrence with direct connections between tokens.
2. **Multi-Head Attention:**
    - Captures multiple dependencies in parallel.
3. **Positional Encoding:**
    - Encodes sequence order.
4. **Residual Connections and Layer Norm:**
    - Stabilize training and improve gradient flow.




### Q11. Why are Transformers more efficient than RNNs and LSTMs?

- **Parallel Processing:**
    - Processes all tokens simultaneously, unlike sequential processing in RNNs.
- **Scalability:**
    - Handles long sequences efficiently using attention mechanisms.
- **No Recurrence:**
    - Removes the bottleneck of sequential gradient propagation.


### Q12. What are the limitations of Transformers?

























