# Convolutional Neural Networks 

CNN
: use convolution in place of general multp. at least one of their layers 
: data in grid-like topology ( multiple D) 
: as linear operation 
: work with inputs of variable size 

Convolution
: is the weighted average  or smoothed estimation
: is an operation of two functions of a real-valued argument

functions
: input is a mD array of data
: kernel is a mD array of parameters
: output: feature map

![img_1.png](img/im/img_1.png)

CNN use a convolutional operator for extracting data features
: efficient to train
: less parameters


Convolution leverage
: 1. sparse interactions
: 2. parameter sharing
: 3. equivariant representations


sparse interactions - limits the connections to local regions 
: connect only few neurons not all -> essential connections
: The kernel (filter) in a CNN focuses only on a local region
: radically reduces number of network connections
: for reduction of # parameters 

parameter sharing - reduce the number oof unique parameters 
: same filter is applied/reused  
: reduce # of unique parameters
: The same filter/weights is applied to every neuron in the feature map generated by the convolution operation

equivariant representations 
: as input changes the output change in a predictable way 
: help model detect features regardless of where they are in the input 


Deep CNN, major changes
: local receptive fields - learning local patterns 
: pooling  - enhances  spatial invariance 
: shared weights

| **Concept in CNN**         | **Matches With**                  | **Description**                                                                                           | **Difference/Similarity**                                                                 |
|-----------------------------|-----------------------------------|-----------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| **Local Receptive Fields**  | **Sparse Interactions**           | Each neuron connects to only a small region of the input, focusing on localized spatial features.          | Same idea: Both focus on **reducing computation** and learning local patterns.            |
| **Pooling**                 | **Equivariant Representations**   | Summarizes local regions, ensuring invariance to small translations (e.g., max or average pooling).        | Similar: Pooling enhances spatial invariance, aligning with the concept of equivariance.  |
| **Shared Weights**          | **Parameter Sharing**             | A single filter is applied across all input locations, reducing the number of parameters significantly.    | Same idea: Both refer to **reusing the same parameters** for efficiency and consistency.  |


CNN vs Dense/fully connected 
: CNN: only few N are connected 
: Dense: all N connect to all N


kernels
: each learn a different feature e.g. edge, corners 
: for each, produce a different feature map 
: different sets of weights
: stack all feature maps into a tensor
: so output is a tensor 
: depth of feature map = # of conv filters 
: kernel smaller than the input

Features
: from Low to high -level features 
: low-level  extract local features 
: high-level  extract global patters

CNN architecture - types of layers 
: Convolutional : detecting local features through filters 
: non-linear: ReLU normalization 
: Pooling : merging similar features 

Pooling
: reduce the spatial size of the feature map 
: so reduce parameters and prevent overfitting
: max pooling: max output within a rectangular 
: average pooling : average -||- 
: is a subsampling step -> statistic of nearby 

how pooling prevent overfitting
: dimensionality reduction ->> reduce parameters -> reduce complexity -> less likely to memorize the training data
: translation invariance ->> focus on more important features (max or average) ->> making the model less sensitive to exact positions 
: noise reduction ->> smooth the data and eliminate minor fluctuations or noise -> 

typical architecture 
: Conv ->  ReLU -> Pooling 
: conv: each region yields a feature map 
: ReLU: feature maps are trained with neurons and weights 
: Pooling : combination of larger regions 

![img.png](img/i/img.png)

Hyper-parameters 
: # of kernels , size of kernels , 
: padding , activation function 

Training Algorithm
: SGD Stochastic Gradient Descent

convolution in layer 
: dot product between parameters of the filter and input
: key : local connectivity & shared weights 

Fully connected layers close to  the end 
: with sigmoid activation function 
: for classification  

last layer: 
: softmax : classification , output : probabilities 
: or 
: regression : linear layer , output : real number 

Flatten
: input into a vector
: before dense 

Dropout
: similar as with dense layer 


translation invariance:
: whether a feature is present  rather exactly where 
: pooling produce it 


Deep Convolutional Layers
: Extract high-level, abstract features.
: More sophisticated (larger receptive fields, advanced structures).
: Deeper in the model.

---
from lecture 4 for CNN 

Feature Map
: Refers to the output of a convolutional layer before applying the activation function
: Represents the response of the convolutional filters to different parts of the input image.

Activation Map
: output after applying the activation function to the feature map.


![img.png](img/im/img.png)




















































