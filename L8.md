### Q1. What is the main difference between RNNs and memory-less models for sequences?
**Memory-Less Models:**
- Process input sequences without retaining prior state information.
- Use fixed-length windows for context
**RNNs:**
    - Maintain hidden states to capture temporal dependencies.
    - Designed to handle variable-length sequences

### Q2. What are some key applications of RNNs?
- **Time-Series Analysis:** 
- **Natural Language Processing (NLP):** Machine translation, sentiment analysis, and text generation.
- **Speech Recognition:** 
- **Image and Video Processing:** 


### Q3. How does an RNN process sequential data?
1. Receives input at each time step.
2. Combines input with the previous hidden state to update the current hidden state.
3. Produces an output for each time step or at the end of the sequence.


### Q4. What are the key challenges faced by RNNs?
- **Vanishing/Exploding Gradients:** 
- **High Computational Cost:** 
- **Sensitivity to Initialization:** 
- **Limited Memory:** Struggles with capturing long-term dependencies.



### Q5. How does unrolling help in understanding RNNs?
**Unrolling:** Visualizes the RNN as a sequence of the same neural network repeated across time steps.
- Highlights shared parameters across time and the flow of information.
  have fewer parameters due to parameter sharing across time steps.
  Exactly! RNNs can be computationally intensive but not memory-intensive,

### Q6. How do LSTMs overcome the vanishing gradient problem in RNNs?
Use gates to add rather than multiply updates to cell states. , allows them to retain and propagate information over long sequences.
1. **Forget Gate:** Removes irrelevant information.
2. **Input Gate:** Adds new relevant information.
3. **Output Gate:** Controls the final output from the cell state.
- **Activation Functions:** Sigmoid and tanh stabilize gradients and regulate updates.



### Q7. What are the differences between vanilla RNNs, LSTMs, ?
**Memory**        | Single hidden state.               | Separate cell and hidden states.
**Efficiency**    | Lightweight but unstable training. | Stable but computationally heavier.
**Applications**  | Short-term dependencies.           | Long-term dependencies.  
**Vanishing Gradient** | Severe problem due to repeated tanh activations. | Solved by additive updates to cell state. |




### Q8. What is sequence labeling, and how is it implemented in RNNs?
Assigning a label to each element in a sequence
1. Pass input through RNN layers.
2. Use a sequence-level loss function like cross-entropy.
3. Backpropagate gradients using BPTT.




### Q9. How does truncated backpropagation through time (TBPTT) work?
Processes sequences in chunks to reduce memory usage and computational cost.
1. Forward pass through a fixed number of time steps (\(k_1\)).
2. Backpropagate gradients for a smaller range of steps (\(k_2\)).
3. Update weights and move to the next chunk.


### Q10. What are bidirectional RNNs, and why are they used?

Process sequences in both forward and backward directions.
Forward Direction: Understands dependencies based on prior elements in the sequence.
Backward Direction: Considers future elements to enhance understanding.



### Q11. How do self-supervised learning approaches train RNNs?

Model predicts the next term in the sequence using the current term.
- Input: "The cat is on the".
- Target: "cat is on the mat".
- **Key Advantage:**
    - No need for labeled data; generates its own targets.


### Q12. What are the core components of an LSTM cell?
1. **Cell State (\(C_t\)):**
    - Stores long-term memory.
    - Passes information with minimal changes across time steps.
2. **Hidden State (\(h_t\)):**
    - Encodes short-term memory.
    - Used to calculate the output at each step.
3. **Gates:**
    - **Forget Gate:** Decides what information to discard.
    - **Input Gate:** Determines what new information to add.
    - **Output Gate:** Filters what to output based on cell state.

    

### Q15. How do LSTMs solve the vanishing gradient problem?

1. **Additive Gradient Flow:**
    - Cell state updates are additive, preventing gradients from shrinking exponentially.
2. **Gate Control:**
    - Forget and input gates regulate gradient flow and prevent information overload.
3. **Tanh and Sigmoid Activations:**
    - Control values in the range \([-1, 1]\) and \([0, 1]\), stabilizing the gradients.



### Q18. Why are forget gates critical in LSTMs?
- Prevent the cell state from accumulating irrelevant information.
- Act as a selective memory filter, allowing only important data to pass.


### Q20. What are some limitations of LSTMs?
1. **Computational Overhead:**
    - due to gates and internal mechanisms.
2. **Training Time:**
    - more epochs than RNNs.
3. **Hyperparameter Sensitivity:**
    - Sensitive to learning rate, batch size, and initialization.
4. **Parallelization Difficulty:**
    - Sequential nature limits parallel computation.


