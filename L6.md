# Lecture 6 


### Q1. What is the difference between training and inference in neural networks?
- training :   
  - Involves backpropagation and updating parameters using stochastic gradient descent (SGD).
- inference :
  - Uses the trained model to predict classes for new input data.


### Q2. Why do deep architectures face training instability, and how is it mitigated?
- vanishing/exploding gradients
  - Solutions: 
    - dropout : 
      - randomly disabling neurons 
    - batch normalization :
      - stabilizing training 
      - reduce sensitivity to initialization 
    - skip connections- residual connection:
      -  shortcut that bypass layers
      - ensure flow even gradients shrink

### Q3. What is data augmentation, and why is it used?
- Enhances training data by applying transformations
- Improves model generalization.
- Avoids overfitting by exposing the model to diverse training data.

### Q4. What are the two types of transfer learning, and when are they used?
**Fine-Tuning:** - Updates pre-trained weights progressively.
**Feature Extraction:**   - Uses pre-trained model weights as fixed feature extractors.

### Q5. What is the role of multitask learning (MTL)?
- Trains a model to perform multiple related tasks simultaneously.
- Improves generalization.
- Reduces overfitting when data is limited.

### Q6. How do residual networks (ResNets) solve the vanishing gradient problem?
- Introduce **residual blocks** with skip (shortcut) connections.
- Formula: \( F(x) = H(x) - x \), which rewrites as \( H(x) = F(x) + x \).
- Allow information to bypass failing layers.
- Preserve gradient flow in very deep networks.

### Q7. What are the advantages of inception-based networks like GoogleNet?

1. Multi-scale convolutions capture features at various granularities.
2. 1x1 convolutions reduce dimensionality, improving efficiency.
3. Filter concatenation merges diverse feature representations.
- Computational efficiency.
- Scalability 
- improved accuracy



### Question: What are the key novelties introduced in AlexNet, VGGNet, GoogleNet, ResNet, and LeNet?
**1. LeNet (1998):**
  - Alternated convolution and pooling layers for structured feature extraction.

**2. AlexNet (2012):**
  -  Introduced ReLU activation for faster convergence.
  - Pioneered dropout regularization to prevent overfitting.

**3. VGGNet (2014):**
  - Use of Small Convolutional Filters:
  - Increased depth for better feature representation.

**4. GoogleNet (Inception, 2014):**
  - Introduced Inception modules with multi-scale convolutions.
  - Used (1 x 1) convolutions for dimensionality reduction.

**5. ResNet (2015):**
  - Introduced residual connections to address gradient issues in deep networks.


### Q9. What is the role of batch normalization?
  - Normalizes intermediate layer activations to stabilize training.
    1. Reduces vanishing/exploding gradients.
    2. Speeds up convergence, allowing higher learning rates.
  - COnv-> BN-> Actv. -> Pooling
    Computes the mean and variance of the activations for each mini-batch.
    Normalizes activations
    Applies learnable parameters to scale and shift







