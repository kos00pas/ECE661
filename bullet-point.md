# Lecture 1 
* dense - fully connected 
* activation functions 
* backpropagation 
* optimization 
* loss function 
* gradient descent 
* ensamble learning 
  * bagging 
  * boosting 
* pooling 
* softmax 

# Lecture 2
* hebbian learning 
* GD 
* stochastic GD 
* mini-batch SGD 
* activation 
  * sigmoid 
  * tangent 
  * ReLU
  * softmax
* backpropagation 


# lecture 3
* backpropagation
* convex 
* gradient
* vecotr-norm-matrices-eigen decomposition 
* derivatives-hessian/jacobian matrix


# lecture 6

* 

